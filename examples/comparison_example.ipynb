{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARISON MODULE\n",
    "\n",
    "This notebook shows how to use the spiketoolkit.comparison module to:\n",
    "  1. compare pair of spike sorters\n",
    "  2. compare multiple spike sorters\n",
    "  3. extract units in agreement with multiple sorters (consensus-based)\n",
    "  4. run systematic performance comparisons on ground truth recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spiketoolkit as st\n",
    "import spikeextractors as se\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a toy example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording, sorting_true = se.example_datasets.toy_example(duration=60, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Compare two spike sorters\n",
    "\n",
    "First, we will run two spike sorters and compare their ouput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_KL = st.sorters.run_klusta(recording)\n",
    "sorting_MS4 = st.sorters.run_mountainsort4(recording)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compare_two_sorters` function allows us to compare the spike sorting output. It returns a `SortingComparison` object, with methods to inspect the comparison output easily. \n",
    "The comparison first matches the units by comparing the agreement between unit spike trains.\n",
    "\n",
    "Let's see how to inspect and access this matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_KL_MS4 = st.comparison.compare_two_sorters(sorting1=sorting_KL, sorting2=sorting_MS4, \n",
    "                                               sorting1_name='klusta', sorting2_name='ms4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check which units were matched, the `get_mapped_sorting` methods can be used. If units are not matched they are listed as -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# units matched to klusta units\n",
    "mapped_sorting_klusta = cmp_KL_MS4.get_mapped_sorting1()\n",
    "print('Klusta units:', sorting_KL.get_unit_ids())\n",
    "print('Klusta mapped units:', mapped_sorting_klusta.get_mapped_unit_ids())\n",
    "\n",
    "# units matched to ms4 units\n",
    "mapped_sorting_ms4 = cmp_KL_MS4.get_mapped_sorting2()\n",
    "print('Mountainsort units:',sorting_MS4.get_unit_ids())\n",
    "print('Mountainsort mapped units:',mapped_sorting_ms4.get_mapped_unit_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_unit_spike_train` returns the mapped spike train. We can use it to check the spike times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that matched spike trains correspond\n",
    "plt.plot(sorting_KL.get_unit_spike_train(7), \n",
    "         np.zeros(len(sorting_KL.get_unit_spike_train(7))), '|')\n",
    "plt.plot(mapped_sorting_klusta.get_unit_spike_train(7),\n",
    "         np.ones(len(mapped_sorting_klusta.get_unit_spike_train(7))), '|')\n",
    "\n",
    "print('Klusta spike train:', sorting_KL.get_unit_spike_train(8)[:10])\n",
    "print('Mountainsort spike trains', mapped_sorting_klusta.get_unit_spike_train(8)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the spike trains are matched, each spike is labelled as:\n",
    "- true positive (tp): spike found both in `sorting1` and `sorting2`\n",
    "- false negative (fn): spike found in `sorting1`, but not in `sorting2`\n",
    "- false positive (fp): spike found in `sorting2`, but not in `sorting1`\n",
    "- misclassification errors (cl): spike found in `sorting1`, not in `sorting2`, found in another matched spike train of `sorting2`, and not labelled as true positives\n",
    "\n",
    "From the counts of these labels the following performance measures ar computed. tp1 refers to `sorting1`, tp2 refers to `sorting2`, while N1 and N2 are the number of spikes in each spike train of `sorting1` and `sorting2`, respectively.\n",
    "- tp rate: #tp1 / N1\n",
    "- fn rate: #fn1 / N1\n",
    "- cl rate: #cl1 / N1\n",
    "- fp rate 1: #fp1 / N1\n",
    "- fp rate 2: #fp1 / N2\n",
    "- accuracy: #tp1 / (#tp1 + #fn1 + #fp1) x 100\n",
    "- sensitivity: #tp1 / (#tp1 + #fn1) x 100\n",
    "- precision: #tp1 / (#tp1 + #fn1) x 100\n",
    "- miss rate: #fn1 / (#tp1 + #fn1) x 100\n",
    "- false discovery rate: #fp1 / (#tp1 + #fp1)\n",
    "\n",
    "The comparison metrics are **biased** towards `sorting1`. In order to get the comparison metrics for `sorting2`, you can invert `sorting1` and `sorting2` in the `compare_two_sorters` function.\n",
    "\n",
    "The `get_performance` method a pandas dataframe (or a dictionary if `output='dict'`) with the comparison metrics. By default, these are calculated for each spike train of `sorting1`, the results can be pooles by average (average of the metrics) and by sum (all counts are summed and the metrics are computed then)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_KL_MS4.get_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_KL_MS4.get_performance(method='pooled_with_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_KL_MS4.get_performance(method='pooled_with_sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Compare multiple spike sorters\n",
    "\n",
    "With 3 or more spike sorters, the comparison is implemented with a graph-based method. The multiple sorter comparison also allows to clean the output by applying a consensus-based method which only selects spike trains and spikes in agreement with multiple sorters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_TDC = st.sorters.run_tridesclous(recording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmp = st.comparison.compare_multiple_sorters(sorting_list=[sorting_KL, sorting_MS4, sorting_TDC], \n",
    "                                              name_list=['KL', 'MS4', 'TDC'], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiple sorters comparison internally computes pairwise comparison, that can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmp.sorting_comparisons['KL']['TDC'].get_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmp.sorting_comparisons['KL']['MS4'].get_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmp.sorting_comparisons['MS4']['TDC'].get_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a better agreement between tridesclous and mountainsort (5 units matched), while klusta only has two matched units with tridesclous, and three with mountainsort.\n",
    "\n",
    "\n",
    "\n",
    "## 3) Consensus-based method\n",
    "\n",
    "We can pull the units in agreement with different sorters using the `get_agreement_sorting` method. This allows to make spike sorting more robust by integrating the output of several algorithms. On the other hand, it might suffer from weak performance of single algorithms.\n",
    "\n",
    "When extracting the units in agreement, the spike trains are modified so that only the true positive spikes between the comparison with the best match are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agr_3 = mcmp.get_agreement_sorting(minimum_matching=3)\n",
    "print('Units in agreement for all three sorters: ', agr_3.get_unit_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agr_2 = mcmp.get_agreement_sorting(minimum_matching=2)\n",
    "print('Units in agreement for at least sorters: ', agr_2.get_unit_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agr_all = mcmp.get_agreement_sorting()\n",
    "print('All units found: ', agr_all.get_unit_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spike trains are cleaned so that only true positives remain. For example, we can see from the Klusta-Tridesclous performance that Klusta unit 4 has many false positives. Let's see which Mountainsort and Tridesclus unit it corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MS4 unit: ', mcmp.sorting_comparisons['KL']['MS4'].get_mapped_sorting1().get_mapped_unit_ids(4))\n",
    "print('TDC unit: ', mcmp.sorting_comparisons['KL']['TDC'].get_mapped_sorting1().get_mapped_unit_ids(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unit index of the different sorters can also be retrieved from the agreement sorting object (`agr_3`) property `sorter_unit_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agr_3.get_unit_property_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agr_3.get_unit_property(4, 'sorter_unit_ids'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we found our unit, we can plot a rasters with the spike trains of the single sorters and the one from the consensus based method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(sorting_KL.get_unit_spike_train(5), \n",
    "         0*np.ones(len(sorting_KL.get_unit_spike_train(5))), '|')\n",
    "plt.plot(sorting_MS4.get_unit_spike_train(4), \n",
    "         1*np.ones(len(sorting_MS4.get_unit_spike_train(4))), '|')\n",
    "plt.plot(sorting_TDC.get_unit_spike_train(0), \n",
    "         2*np.ones(len(sorting_TDC.get_unit_spike_train(0))), '|')\n",
    "plt.plot(agr_3.get_unit_spike_train(4), \n",
    "         3*np.ones(len(agr_3.get_unit_spike_train(4))), '|')\n",
    "\n",
    "print('Klusta spike train length', len(sorting_KL.get_unit_spike_train(5)))\n",
    "print('Mountainsort spike train length', len(sorting_MS4.get_unit_spike_train(4)))\n",
    "print('Tridesclous spike train length', len(sorting_TDC.get_unit_spike_train(0)))\n",
    "print('Agreement spike train length', len(agr_3.get_unit_spike_train(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the best match is between Mountainsort and Tridesclous, but only the true positive spikes make up the agreement spike train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Run systematic performance comparison\n",
    "\n",
    "This part of the notebook illustrates how to run systematic performance comparisons on ground truth recordings\n",
    "\n",
    "This will be done with mainly with 2 functions:\n",
    "  * **spiketoolkit.sorters.run_sorters** : this run several sorters on serevals dataset\n",
    "  * **spiketoolkit.comparison.gather_sorting_comparison** : this run several all possible comparison\n",
    "    with ground truth and results some metrics (accuracy, true positive rate, ..)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Generate several dataset with \"toy_example\"\n",
    "\n",
    "We first generate two recordings to be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec0, gt_sorting0 = se.example_datasets.toy_example(num_channels=4, duration=30, seed=10)\n",
    "rec1, gt_sorting1 = se.example_datasets.toy_example(num_channels=32, duration=30, seed=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check which spike sorters are available, we can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.sorters.available_sorters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Run several sorters on all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is really verbose due to some sorter so switch off output console\n",
    "\n",
    "recording_dict = {'toy_tetrode' : rec0, 'toy_probe32': rec1}\n",
    "sorter_list = ['klusta', 'spykingcircus', 'tridesclous', 'herdingspikes']\n",
    "path = Path('comparison_example')\n",
    "working_folder = path / 'working_folder'\n",
    "if working_folder.is_dir():\n",
    "    shutil.rmtree(str(working_folder))\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "st.sorters.run_sorters(sorter_list, recording_dict, working_folder, engine=None)\n",
    "t1 = time.perf_counter()\n",
    "print('total run time', t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Collect dataframes for comparison\n",
    "\n",
    "As shown previously, the performance is returned as a pandas dataframe. The `gather_sorting_comparison` function, gathers all the outputs in the working folder and merges them in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths = {'toy_tetrode': gt_sorting0, 'toy_probe32': gt_sorting1}\n",
    "\n",
    "comp, dataframes = st.comparison.gather_sorting_comparison(working_folder, ground_truths, use_multi_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Display comparison tables\n",
    "\n",
    "Pandas dataframes can be nicely displayed as tables in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['perf_pooled_with_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['perf_pooled_with_average']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['run_times']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Easy plot with seaborn\n",
    "\n",
    "Seaborn allows to easily plot pandas dataframes. Let's see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_times = dataframes['run_times'].reset_index()\n",
    "fig, ax = plt.subplots()\n",
    "sn.barplot(data=run_times, x='rec_name', y='run_time', hue='sorter_name', ax=ax)\n",
    "ax.set_title('Run times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs = dataframes['perf_pooled_with_average'].reset_index()\n",
    "fig, ax = plt.subplots()\n",
    "sn.barplot(data=perfs, x='rec_name', y='tp_rate', hue='sorter_name', ax=ax)\n",
    "ax.set_title('True positive rate')\n",
    "ax.set_ylim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs = dataframes['perf_pooled_with_sum'].reset_index()\n",
    "fig, ax = plt.subplots()\n",
    "ax = sn.barplot(data=perfs, x='rec_name', y='accuracy', hue='sorter_name', ax=ax)\n",
    "ax.set_title('accuracy')\n",
    "ax.set_ylim(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook showed the capabilities of `spiketoolkit` to perform pair-wise comparisons between spike sorting outputs, comparisons among multiple sorters and consensus-based spike sorting, and systematic comparisons for grount-truth data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
